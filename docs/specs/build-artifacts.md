# Build Artifacts Specification

## Overview

`streamt build` generates deployable artifacts without applying them. This enables debugging, auditing, air-gapped deployments, and advanced workflows outside the standard GitOps model.

---

## Design Principles

1. **Git remains source of truth** — Artifacts are derived, not authoritative
2. **Reproducible** — Same inputs = same outputs (deterministic)
3. **Self-contained** — Artifact has everything needed to deploy
4. **Inspectable** — Human-readable where possible

---

## CLI Commands

### Build Artifacts

```bash
# Build for specific environment
streamt build --env prod --output dist/

# Build with resolved secrets (for air-gapped transfer)
streamt build --env prod --output dist/ --resolve-secrets

# Build without secrets (default, safer)
streamt build --env prod --output dist/
```

### Deploy from Artifact

```bash
# Deploy pre-built artifact
streamt deploy dist/ --env prod --confirm

# Dry-run: show what would be deployed
streamt deploy dist/ --env prod --dry-run
```

### Inspect Artifact

```bash
# Show artifact contents
streamt artifact inspect dist/

# Show specific component
streamt artifact inspect dist/ --component flink
streamt artifact inspect dist/ --component topics
```

---

## Artifact Structure

```
dist/
├── manifest.json           # Metadata, checksums, build info
├── topics/
│   ├── events.raw.v1.json
│   └── events.clean.v1.json
├── schemas/
│   ├── events-raw-value.avsc
│   └── events-clean-value.avsc
├── flink/
│   ├── events_clean.sql    # Complete Flink SQL (CREATE + INSERT)
│   └── hourly_stats.sql
├── connectors/
│   └── postgres_sink.json  # Kafka Connect config
└── gateway/                # If using Conduktor Gateway
    └── virtual_topics.json
```

### manifest.json

```json
{
  "version": "1.0.0",
  "build_time": "2025-01-15T10:30:00Z",
  "project": {
    "name": "payments-pipeline",
    "version": "2.1.0"
  },
  "environment": "prod",
  "git": {
    "commit": "abc123def456",
    "branch": "main",
    "dirty": false
  },
  "checksums": {
    "topics/events.raw.v1.json": "sha256:...",
    "flink/events_clean.sql": "sha256:..."
  },
  "components": {
    "topics": ["events.raw.v1", "events.clean.v1"],
    "flink_jobs": ["events_clean", "hourly_stats"],
    "connectors": ["postgres_sink"],
    "schemas": ["events-raw-value", "events-clean-value"]
  },
  "secrets_resolved": false
}
```

---

## Use Cases

### 1. Debugging Generated SQL

```bash
# What SQL will be sent to Flink?
streamt build --env dev --output debug/
cat debug/flink/events_clean.sql
```

Output:
```sql
-- Generated by streamt v0.1.0
-- Model: events_clean
-- Environment: dev

CREATE TABLE `source_events` (
  `event_id` STRING,
  `user_id` STRING,
  `event_type` STRING,
  `timestamp` TIMESTAMP(3)
) WITH (
  'connector' = 'kafka',
  'topic' = 'events.raw.v1',
  'properties.bootstrap.servers' = 'dev-kafka:9092',
  ...
);

CREATE TABLE `sink_events_clean` (...) WITH (...);

INSERT INTO `sink_events_clean`
SELECT event_id, user_id, event_type
FROM `source_events`
WHERE event_id IS NOT NULL;
```

### 2. Air-Gapped Deployment

```bash
# On CI server (has network access)
streamt build --env prod --output artifact.tar.gz --resolve-secrets

# Transfer to air-gapped environment
scp artifact.tar.gz prod-bastion:

# On prod bastion (no git, no source)
streamt deploy artifact.tar.gz --confirm
```

### 3. Audit Trail

```bash
# Store artifacts alongside releases
streamt build --env prod --output releases/v2.1.0/

# Later: what exactly was deployed in v2.1.0?
streamt artifact inspect releases/v2.1.0/
```

### 4. Pre-flight Inspection

```bash
# Before deploying, review everything
streamt build --env prod --output review/
streamt artifact inspect review/

# Team reviews generated configs, then:
streamt deploy review/ --env prod --confirm
```

---

## Secret Handling

### Default: Secrets as References

```json
// topics/events.raw.v1.json
{
  "name": "events.raw.v1",
  "config": {
    "bootstrap.servers": "${KAFKA_BOOTSTRAP}"
  }
}
```

Secrets resolved at `deploy` time from environment.

### With --resolve-secrets

```json
// topics/events.raw.v1.json
{
  "name": "events.raw.v1",
  "config": {
    "bootstrap.servers": "prod-kafka.example.com:9092"
  }
}
```

**Warning**: Artifact contains secrets. Handle securely.

---

## Validation

### Build-time Validation

```bash
streamt build --env prod --output dist/
# Validates:
# - All references resolve
# - Schema compatibility
# - Naming conventions
# - Governance rules
```

### Deploy-time Validation

```bash
streamt deploy dist/ --env prod
# Validates:
# - Artifact integrity (checksums)
# - Environment matches
# - No drift from expected state (optional)
```

---

## Comparison with Plan

| Aspect | `streamt plan` | `streamt build` |
|--------|----------------|-----------------|
| **Output** | Diff (what changes) | Full content (what deploys) |
| **Purpose** | Review changes | Debug/audit/transfer |
| **Needs infra access** | Yes (reads current state) | No |
| **Deployable** | No | Yes (via `deploy`) |

---

## Implementation Phases

### Phase 1: Basic Build
- [ ] `streamt build --env <name> --output <dir>`
- [ ] Generate topics/*.json
- [ ] Generate flink/*.sql
- [ ] Generate manifest.json with checksums
- [ ] `streamt artifact inspect <dir>`

### Phase 2: Deploy from Artifact
- [ ] `streamt deploy <artifact> --env <name>`
- [ ] Validate checksums
- [ ] Support both directory and .tar.gz

### Phase 3: Advanced Features
- [ ] `--resolve-secrets` flag
- [ ] Git metadata in manifest
- [ ] Schema files export
- [ ] Connector configs export

### Phase 4: Integration
- [ ] `streamt build` in CI templates
- [ ] Artifact storage recommendations (S3, GCS)
- [ ] Artifact signing (GPG/cosign)

---

## Non-Goals

- **Artifact as source of truth** — Git is the source of truth, always
- **Cross-environment artifacts** — Build is env-specific (different configs)
- **Incremental artifacts** — Each build is complete (not a delta)

---

## Open Questions

1. **Artifact format**: Directory vs tarball vs both?
2. **Compression**: gzip, zstd, none?
3. **Signing**: GPG, cosign, or defer to CI/CD tooling?
4. **State inclusion**: Include current state snapshot for offline plan?
