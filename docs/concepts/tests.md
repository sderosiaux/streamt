---
title: Tests
description: Validate data quality with schema, sample, and continuous tests
---

# Tests

Tests validate that your streaming data meets quality expectations. streamt supports three test types for different validation needs.

## Test Types

| Type | When | How | Use Case |
|------|------|-----|----------|
| `schema` | Compile time | Validates structure | Schema constraints |
| `sample` | On demand | Consumes N messages | Data quality checks |
| `continuous` | Always running | Flink monitoring job | Real-time alerting |

## Schema Tests

Schema tests validate at compile time without consuming messages:

```yaml
tests:
  - name: orders_schema
    model: orders_clean
    type: schema
    assertions:
      - not_null:
          columns: [order_id, customer_id, amount]

      - accepted_values:
          column: status
          values: [pending, confirmed, shipped]
```

Run with:

```bash
streamt test --type schema
```

## Sample Tests

Sample tests consume actual messages and validate assertions:

```yaml
tests:
  - name: orders_quality
    model: orders_clean
    type: sample
    sample_size: 1000        # Consume 1000 messages
    timeout: 30s             # Max wait time

    assertions:
      - range:
          column: amount
          min: 0
          max: 1000000

      - unique_key:
          key: order_id
          tolerance: 0.01    # Allow 1% duplicates
```

Run with:

```bash
streamt test --type sample
```

## Continuous Tests

Continuous tests deploy as Flink jobs for real-time monitoring:

```yaml
tests:
  - name: orders_freshness
    model: orders_clean
    type: continuous

    assertions:
      - max_lag:
          seconds: 300       # Alert if lag > 5 minutes

      - throughput:
          min_per_minute: 100

    on_failure:
      - alert:
          channel: slack
          webhook: ${SLACK_WEBHOOK}
```

Deploy with:

```bash
streamt test --deploy
```

## Assertions Reference

**Status Legend:**
- âœ… **Implemented** â€” Available in continuous tests
- ðŸš§ **Planned** â€” Documented but not yet implemented

### not_null âœ…

Check that columns are never null:

```yaml
- not_null:
    columns: [order_id, customer_id, amount]
```

### accepted_values âœ…

Check that a column only contains allowed values:

```yaml
- accepted_values:
    column: status
    values: [pending, confirmed, shipped, delivered, cancelled]
```

### accepted_types âœ…

Validate column data types:

```yaml
- accepted_types:
    types:
      order_id: string
      amount: number
      created_at: timestamp
```

**Supported types:** `string`, `number`, `int`, `bigint`, `boolean`, `timestamp`, `date`, `time`

### range âœ…

Check numeric values are within bounds:

```yaml
- range:
    column: amount
    min: 0
    max: 1000000
```

### unique_key ðŸš§

Validate uniqueness of a key:

```yaml
- unique_key:
    key: order_id
    tolerance: 0.0      # 0% duplicates allowed
```

Or with tolerance:

```yaml
- unique_key:
    key: transaction_id
    tolerance: 0.01     # Allow 1% duplicates (for exactly-once issues)
```

### foreign_key ðŸš§

Check referential integrity:

```yaml
- foreign_key:
    column: customer_id
    references:
      model: customers
      column: id
```

### max_lag ðŸš§

Monitor consumer lag (continuous only):

```yaml
- max_lag:
    seconds: 300        # 5 minutes max lag
```

### throughput ðŸš§

Monitor message throughput (continuous only):

```yaml
- throughput:
    min_per_minute: 100
    max_per_minute: 10000
```

### distribution ðŸš§

Check value distribution:

```yaml
- distribution:
    column: country
    expected:
      US: 0.4           # ~40% US
      EU: 0.3           # ~30% EU
      APAC: 0.3         # ~30% APAC
    tolerance: 0.1      # Allow 10% variance
```

### custom_sql âœ…

Write custom validation with a WHERE clause:

```yaml
- custom_sql:
    name: positive_balance
    where: CAST(`balance` AS DOUBLE) < 0
    detail_column: account_id
```

**Note:** For continuous tests, provide a `where` condition that identifies violations. The `detail_column` specifies which column value to include in violation details.

## Failure Actions

Define what happens when tests fail:

### Alert

Send notifications:

```yaml
on_failure:
  - alert:
      channel: slack
      webhook: ${SLACK_WEBHOOK}
      message: "Data quality issue in {{ model.name }}"
```

Supported channels:

- `slack` â€” Slack webhook
- `pagerduty` â€” PagerDuty events
- `webhook` â€” Custom HTTP webhook
- `email` â€” Email notification

### Pause Model

Pause the model's processing:

```yaml
on_failure:
  - pause:
      model: orders_clean
```

### Route to DLQ

Send failing records to a dead letter queue:

```yaml
on_failure:
  - dlq:
      topic: orders.dlq.v1
      include_error: true
```

### Block Deployment

Prevent deployment if tests fail:

```yaml
on_failure:
  - block_deploy: true
```

## Complete Test Example

```yaml title="tests/orders_comprehensive.yml"
tests:
  # Schema validation (compile-time)
  - name: orders_schema_validation
    model: orders_clean
    type: schema
    description: Validate orders schema constraints

    assertions:
      - not_null:
          columns: [order_id, customer_id, amount, status]

      - accepted_values:
          column: status
          values: [pending, confirmed, shipped, delivered, cancelled]

      - accepted_values:
          column: currency
          values: [USD, EUR, GBP, JPY]

  # Sample test (on-demand)
  - name: orders_data_quality
    model: orders_clean
    type: sample
    sample_size: 5000
    description: Validate data quality on sample

    assertions:
      - range:
          column: amount
          min: 1
          max: 10000000

      - unique_key:
          key: order_id
          tolerance: 0.001

      - distribution:
          column: status
          expected:
            pending: 0.2
            confirmed: 0.5
            shipped: 0.2
            delivered: 0.1
          tolerance: 0.15

  # Continuous monitoring (always-on)
  - name: orders_monitoring
    model: orders_clean
    type: continuous
    description: Real-time monitoring for orders pipeline

    assertions:
      - max_lag:
          seconds: 180

      - throughput:
          min_per_minute: 50
          max_per_minute: 5000

    on_failure:
      - alert:
          channel: slack
          webhook: ${SLACK_ALERTS_WEBHOOK}
          message: |
            :warning: Orders pipeline issue detected!
            Model: {{ model.name }}
            Test: {{ test.name }}
            Assertion: {{ assertion.name }}
      - alert:
          channel: pagerduty
          routing_key: ${PAGERDUTY_KEY}
          severity: warning
```

## Running Tests

### Run All Tests

```bash
streamt test
```

### Filter by Type

```bash
streamt test --type schema
streamt test --type sample
streamt test --type continuous
```

### Filter by Model

```bash
streamt test --model orders_clean
```

### Deploy Continuous Tests

```bash
streamt test --deploy
```

### Check Test Status

```bash
streamt status --tests
```

## Test Output

```bash
$ streamt test

Running tests...

Schema Tests:
  âœ“ orders_schema_validation (3 assertions)
  âœ“ customers_schema_validation (2 assertions)

Sample Tests:
  âœ“ orders_data_quality (1000 messages, 4 assertions)
  âœ— orders_uniqueness - FAILED
    â†’ unique_key: order_id has 2.3% duplicates (tolerance: 0.1%)

Continuous Tests:
  â—‹ orders_monitoring - DEPLOYED (Flink job: running)
  â—‹ revenue_alerting - DEPLOYED (Flink job: running)

Results: 3 passed, 1 failed, 2 deployed
```

## Best Practices

### 1. Layer Your Tests

```yaml
# Layer 1: Schema (fast, always run)
- type: schema
  assertions: [not_null, accepted_values]

# Layer 2: Sample (medium, run periodically)
- type: sample
  assertions: [range, unique_key, distribution]

# Layer 3: Continuous (slow, always-on monitoring)
- type: continuous
  assertions: [max_lag, throughput]
```

### 2. Set Meaningful Thresholds

```yaml
# Consider your data characteristics
- unique_key:
    key: event_id
    tolerance: 0.001   # 0.1% for high-volume with rare duplicates

- throughput:
    min_per_minute: 100   # Based on actual expected volume
```

### 3. Test Critical Paths

Focus tests on business-critical data:

```yaml
# Test payment data rigorously
- name: payments_critical
  model: payments_validated
  assertions:
    - not_null: { columns: [payment_id, amount, status] }
    - range: { column: amount, min: 1 }
    - unique_key: { key: payment_id, tolerance: 0 }
```

### 4. Use DLQ for Bad Data

```yaml
on_failure:
  - dlq:
      topic: payments.dlq.v1
      include_error: true
      include_metadata: true
```

### 5. Alert Appropriately

```yaml
# Warning: Slack
# Critical: PagerDuty
on_failure:
  - alert: { channel: slack, ... }       # First notification
  - alert: { channel: pagerduty, ... }   # Escalation
```

## Next Steps

- [Exposures](exposures.md) â€” Document downstream consumers
- [Governance Rules](../reference/governance.md) â€” Enforce test requirements
- [CLI Reference](../reference/cli.md) â€” Test command options
